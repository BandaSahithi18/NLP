{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BandaSahithi18/NLP/blob/main/2403A54070_ASS_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK1:LDA - SAMPLE DATA"
      ],
      "metadata": {
        "id": "wd-V_pnm8-15"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cldJzTD_5qxL"
      },
      "source": [
        "step1:load the data set using pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJDwqXfY3qYw",
        "outputId": "555ed0bb-08cf-4e65-d1d9-383f45721108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                             News\n",
            "0   Virat scored century in match\n",
            "1            BJP won in elections\n",
            "2  Bumra took 5 wicket in a match\n",
            "3  Congress form state government\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df=pd.read_excel('/content/LDA-Data.xlsx')\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br9CvD3w5yuf"
      },
      "source": [
        "step2:Text preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AujUjT63Mku",
        "outputId": "e50b1aff-8891-43d4-8c5c-ac75020b46c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                             News                  processed_news\n",
            "0   Virat scored century in match   virat scored century in match\n",
            "1            BJP won in elections            bjp won in elections\n",
            "2  Bumra took 5 wicket in a match  bumra took 5 wicket in a match\n",
            "3  Congress form state government  congress form state government\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset from the available Excel file\n",
        "df = pd.read_excel('/content/LDA-Data.xlsx')\n",
        "\n",
        "# Define preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove URLs (http, https, www)\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove social media mentions (@username)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Remove hashtags (#hashtag)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove emojis\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    # Remove special characters (keep only alphanumeric and spaces)\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "\n",
        "    # Normalize whitespace (reduce multiple spaces to single space)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to 'News' column from the loaded Excel data\n",
        "df['processed_news'] = df['News'].apply(preprocess_text)\n",
        "\n",
        "# Preview results\n",
        "print(df[['News', 'processed_news']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU-nOMhG5-GB"
      },
      "source": [
        "word tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ru57u6CJ5oPm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07c2b37b-51d6-403d-998f-09a1a53ccc9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   processed_news                          tokenized_news\n",
            "0   virat scored century in match     [virat, scored, century, in, match]\n",
            "1            bjp won in elections               [bjp, won, in, elections]\n",
            "2  bumra took 5 wicket in a match  [bumra, took, 5, wicket, in, a, match]\n",
            "3  congress form state government     [congress, form, state, government]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')  # required in newer NLTK versions\n",
        "\n",
        "df['tokenized_news'] = df['processed_news'].apply(lambda x: word_tokenize(x))\n",
        "print(df[['processed_news', 'tokenized_news']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2trc8xk6TyJ"
      },
      "source": [
        "stop word removal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUspO6Ar6Lrk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aa83857-05ee-498a-ee7d-00354080da98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           tokenized_news                        filtered_news\n",
            "0     [virat, scored, century, in, match]      [virat, scored, century, match]\n",
            "1               [bjp, won, in, elections]                     [bjp, elections]\n",
            "2  [bumra, took, 5, wicket, in, a, match]      [bumra, took, 5, wicket, match]\n",
            "3     [congress, form, state, government]  [congress, form, state, government]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Ensure the stopwords resource is available\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define the set of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords from tokenized_news\n",
        "df['filtered_news'] = df['tokenized_news'].apply(\n",
        "    lambda tokens: [w for w in tokens if w.lower() not in stop_words]\n",
        ")\n",
        "\n",
        "# Preview results\n",
        "print(df[['tokenized_news', 'filtered_news']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAM18-iS6hQW"
      },
      "source": [
        "lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z56WmIlb6lct",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66d22ffb-686b-4e21-e40f-24c5a19b7eb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                         filtered_news                      lemmatized_news\n",
            "0      [virat, scored, century, match]      [virat, scored, century, match]\n",
            "1                     [bjp, elections]                      [bjp, election]\n",
            "2      [bumra, took, 5, wicket, match]      [bumra, took, 5, wicket, match]\n",
            "3  [congress, form, state, government]  [congress, form, state, government]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Ensure the WordNet corpus is available\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # optional, improves lemmatization coverage\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply lemmatization to each token list\n",
        "df['lemmatized_news'] = df['filtered_news'].apply(\n",
        "    lambda tokens: [lemmatizer.lemmatize(w) for w in tokens]\n",
        ")\n",
        "\n",
        "# Preview results\n",
        "print(df[['filtered_news', 'lemmatized_news']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXZfONJt6yLz"
      },
      "source": [
        "Rejoin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qM8Ue2z960N_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f66f4739-335c-42f2-a281-02822ba9dd65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       lemmatized_news                      clean_news\n",
            "0      [virat, scored, century, match]      virat scored century match\n",
            "1                      [bjp, election]                    bjp election\n",
            "2      [bumra, took, 5, wicket, match]       bumra took 5 wicket match\n",
            "3  [congress, form, state, government]  congress form state government\n"
          ]
        }
      ],
      "source": [
        "# Rejoin lemmatized words into a single string\n",
        "df['clean_news'] = df['lemmatized_news'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "# Preview results\n",
        "print(df[['lemmatized_news', 'clean_news']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTu3tG1Y4EZT"
      },
      "source": [
        "step5: Apply LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXEZ73VF3Qey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4678e2a-e6ad-4929-865b-2b818479ea0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "\n",
        "from gensim import corpora\n",
        "\n",
        "# Create dictionary\n",
        "dictionary = corpora.Dictionary(df['lemmatized_news'])\n",
        "\n",
        "# Create corpus (bag-of-words representation)\n",
        "corpus = [dictionary.doc2bow(text) for text in df['lemmatized_news']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwM_0lJp31Vq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6ca72f0-b6bb-49f7-ccc7-09c2f451496f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: 0.199*\"bjp\" + 0.199*\"election\" + 0.050*\"century\" + 0.050*\"virat\" + 0.050*\"scored\"\n",
            "Topic 1: 0.154*\"congress\" + 0.154*\"form\" + 0.154*\"state\" + 0.154*\"government\" + 0.039*\"bjp\"\n",
            "Topic 2: 0.171*\"match\" + 0.098*\"bumra\" + 0.098*\"5\" + 0.098*\"wicket\" + 0.098*\"took\"\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import LdaModel\n",
        "\n",
        "# Train LDA model\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=3, random_state=42, passes=10)\n",
        "\n",
        "# Print topics\n",
        "for idx, topic in lda_model.print_topics(num_words=5):\n",
        "    print(f\"Topic {idx}: {topic}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cxkxGk04sce"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l81hdeC-38O3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15adf5fa-1b7f-49cd-afea-3dacdf0d82ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Topic 0:\n",
            "  bjp (0.199)\n",
            "  election (0.199)\n",
            "  century (0.050)\n",
            "  virat (0.050)\n",
            "  scored (0.050)\n",
            "  match (0.050)\n",
            "  took (0.050)\n",
            "  bumra (0.050)\n",
            "  5 (0.050)\n",
            "  wicket (0.050)\n",
            "\n",
            "Topic 1:\n",
            "  congress (0.154)\n",
            "  form (0.154)\n",
            "  state (0.154)\n",
            "  government (0.154)\n",
            "  bjp (0.039)\n",
            "  virat (0.039)\n",
            "  election (0.039)\n",
            "  century (0.039)\n",
            "  match (0.039)\n",
            "  scored (0.039)\n",
            "\n",
            "Topic 2:\n",
            "  match (0.171)\n",
            "  bumra (0.098)\n",
            "  5 (0.098)\n",
            "  wicket (0.098)\n",
            "  took (0.098)\n",
            "  scored (0.097)\n",
            "  virat (0.097)\n",
            "  century (0.097)\n",
            "  election (0.024)\n",
            "  bjp (0.024)\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import LdaModel\n",
        "\n",
        "# Train LDA model (example with 3 topics)\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=3, random_state=42, passes=10)\n",
        "\n",
        "# Print top words for each topic\n",
        "num_words = 10  # how many words per topic\n",
        "topics = lda_model.show_topics(num_topics=3, num_words=num_words, formatted=False)\n",
        "\n",
        "for topic_num, words in topics:\n",
        "    print(f\"\\nTopic {topic_num}:\")\n",
        "    for word, weight in words:\n",
        "        print(f\"  {word} ({weight:.3f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BOW"
      ],
      "metadata": {
        "id": "WDyS7Z-y8SsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform sentences\n",
        "bow_matrix = vectorizer.fit_transform(df['clean_news'])\n",
        "\n",
        "# Convert to array for readability\n",
        "bow_array = bow_matrix.toarray()\n",
        "\n",
        "# Show vocabulary\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "\n",
        "# Show Bag-of-Words matrix\n",
        "print(\"Bag-of-Words Matrix:\\n\", bow_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYoo4UiH7VR1",
        "outputId": "4d7b27d9-1e00-41bd-c9ee-b417847bb78f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['bjp' 'bumra' 'century' 'congress' 'election' 'form' 'government' 'match'\n",
            " 'scored' 'state' 'took' 'virat' 'wicket']\n",
            "Bag-of-Words Matrix:\n",
            " [[0 0 1 0 0 0 0 1 1 0 0 1 0]\n",
            " [1 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 1 0 0 1 0 1]\n",
            " [0 0 0 1 0 1 1 0 0 1 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform sentences\n",
        "bow_matrix = vectorizer.fit_transform(df['clean_news'])\n",
        "\n",
        "# Convert to DataFrame\n",
        "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Add original sentences for reference\n",
        "bow_df.insert(0, \"Sentence\", df['clean_news'])\n",
        "\n",
        "# Display Bag-of-Words DataFrame\n",
        "print(bow_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-JjU8X88rnX",
        "outputId": "279dcbe1-5edb-48ec-f3c6-b0892c8b8d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                         Sentence  bjp  bumra  century  congress  election  \\\n",
            "0      virat scored century match    0      0        1         0         0   \n",
            "1                    bjp election    1      0        0         0         1   \n",
            "2       bumra took 5 wicket match    0      1        0         0         0   \n",
            "3  congress form state government    0      0        0         1         0   \n",
            "\n",
            "   form  government  match  scored  state  took  virat  wicket  \n",
            "0     0           0      1       1      0     0      1       0  \n",
            "1     0           0      0       0      0     0      0       0  \n",
            "2     0           0      1       0      0     1      0       1  \n",
            "3     1           1      0       0      1     0      0       0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "APPLY NMF"
      ],
      "metadata": {
        "id": "tHA5iYtq9z19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import NMF\n",
        "\n",
        "# Choose number of topics\n",
        "num_topics = 2\n",
        "\n",
        "nmf_model = NMF(n_components=num_topics, random_state=42)\n",
        "nmf_model.fit(tfidf_matrix)\n",
        "\n",
        "# Get topic-word matrix\n",
        "topic_words = nmf_model.components_\n"
      ],
      "metadata": {
        "id": "0UHYqSXb9qlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "num_words = 10  # top words per topic\n",
        "\n",
        "for topic_idx, topic in enumerate(topic_words):\n",
        "    print(f\"\\nTopic {topic_idx}:\")\n",
        "    top_indices = topic.argsort()[-num_words:][::-1]\n",
        "    for i in top_indices:\n",
        "        print(f\"  {feature_names[i]} ({topic[i]:.3f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ry0UK6cl94ev",
        "outputId": "e6b4a198-0966-4d36-f6be-9d3f9dc987eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Topic 0:\n",
            "  match (0.715)\n",
            "  bumra (0.453)\n",
            "  wicket (0.453)\n",
            "  took (0.453)\n",
            "  virat (0.453)\n",
            "  scored (0.453)\n",
            "  century (0.453)\n",
            "  state (0.000)\n",
            "  government (0.000)\n",
            "  election (0.000)\n",
            "\n",
            "Topic 1:\n",
            "  election (0.722)\n",
            "  bjp (0.722)\n",
            "  form (0.255)\n",
            "  government (0.255)\n",
            "  state (0.255)\n",
            "  congress (0.255)\n",
            "  took (0.000)\n",
            "  virat (0.000)\n",
            "  wicket (0.000)\n",
            "  scored (0.000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcSi2gIpUbMP"
      },
      "source": [
        "TASK 2: NMF-BOW KAGGLE DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTxENqPY4dNP",
        "outputId": "d9d1a1e1-b5e0-432d-d0ae-fb7dc3506eee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              titles  \\\n",
            "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
            "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
            "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
            "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
            "4  Background-Foreground Segmentation for Interio...   \n",
            "\n",
            "                                           summaries  \\\n",
            "0  Stereo matching is one of the widely used tech...   \n",
            "1  The recent advancements in artificial intellig...   \n",
            "2  In this paper, we proposed a novel mutual cons...   \n",
            "3  Consistency training has proven to be an advan...   \n",
            "4  To ensure safety in automated driving, the cor...   \n",
            "\n",
            "                         terms  \n",
            "0           ['cs.CV', 'cs.LG']  \n",
            "1  ['cs.CV', 'cs.AI', 'cs.LG']  \n",
            "2           ['cs.CV', 'cs.AI']  \n",
            "3                    ['cs.CV']  \n",
            "4           ['cs.CV', 'cs.LG']  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df1=pd.read_csv('/content/arxiv_data.csv')\n",
        "print(df1.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PREPROCESSING"
      ],
      "metadata": {
        "id": "6qchxIDD-7J1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s04y3TerUywS",
        "outputId": "72020b37-9a72-4023-ac54-b6d619cd2ccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           summaries  \\\n",
            "0  Stereo matching is one of the widely used tech...   \n",
            "1  The recent advancements in artificial intellig...   \n",
            "2  In this paper, we proposed a novel mutual cons...   \n",
            "3  Consistency training has proven to be an advan...   \n",
            "4  To ensure safety in automated driving, the cor...   \n",
            "\n",
            "                                      processed_news  \n",
            "0  stereo matching is one of the widely used tech...  \n",
            "1  the recent advancements in artificial intellig...  \n",
            "2  in this paper we proposed a novel mutual consi...  \n",
            "3  consistency training has proven to be an advan...  \n",
            "4  to ensure safety in automated driving the corr...  \n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset from the available CSV file\n",
        "df1 = pd.read_csv('/content/arxiv_data.csv')\n",
        "\n",
        "# Define preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove URLs (http, https, www)\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove social media mentions (@username)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Remove hashtags (#hashtag)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove emojis\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    # Remove special characters (keep only alphanumeric and spaces)\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "\n",
        "    # Normalize whitespace (reduce multiple spaces to single space)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to 'summaries' column from the loaded CSV data\n",
        "df1['processed_news'] = df1['summaries'].apply(preprocess_text)\n",
        "\n",
        "# Preview results\n",
        "print(df1[['summaries', 'processed_news']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOKENIZATION"
      ],
      "metadata": {
        "id": "cOA6NDcs-4Gs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtxE5hiTVFnK",
        "outputId": "439d9dbf-e966-458a-8f88-b2f3c642feab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      processed_news  \\\n",
            "0  stereo matching is one of the widely used tech...   \n",
            "1  the recent advancements in artificial intellig...   \n",
            "2  in this paper we proposed a novel mutual consi...   \n",
            "3  consistency training has proven to be an advan...   \n",
            "4  to ensure safety in automated driving the corr...   \n",
            "\n",
            "                                      tokenized_news  \n",
            "0  [stereo, matching, is, one, of, the, widely, u...  \n",
            "1  [the, recent, advancements, in, artificial, in...  \n",
            "2  [in, this, paper, we, proposed, a, novel, mutu...  \n",
            "3  [consistency, training, has, proven, to, be, a...  \n",
            "4  [to, ensure, safety, in, automated, driving, t...  \n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')  # required in newer NLTK versions\n",
        "\n",
        "df1['tokenized_news'] = df1['processed_news'].apply(lambda x: word_tokenize(x))\n",
        "print(df1[['processed_news', 'tokenized_news']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STOP WORD REMOVAL"
      ],
      "metadata": {
        "id": "JjfDcxw3-_xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords') # Ensure stopwords is downloaded\n",
        "\n",
        "# Tokenize 'processed_news'\n",
        "df1['tokenized_news'] = df1['processed_news'].apply(word_tokenize)\n",
        "\n",
        "# Define the set of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords from tokenized_news\n",
        "df1['filtered_news'] = df1['tokenized_news'].apply(\n",
        "    lambda tokens: [w for w in tokens if w.lower() not in stop_words]\n",
        ")\n",
        "\n",
        "print(df1[['processed_news', 'tokenized_news', 'filtered_news']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbMvgcVeBxur",
        "outputId": "a1ee437f-12c3-4e38-ff78-a989844d074b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      processed_news  \\\n",
            "0  stereo matching is one of the widely used tech...   \n",
            "1  the recent advancements in artificial intellig...   \n",
            "2  in this paper we proposed a novel mutual consi...   \n",
            "3  consistency training has proven to be an advan...   \n",
            "4  to ensure safety in automated driving the corr...   \n",
            "\n",
            "                                      tokenized_news  \\\n",
            "0  [stereo, matching, is, one, of, the, widely, u...   \n",
            "1  [the, recent, advancements, in, artificial, in...   \n",
            "2  [in, this, paper, we, proposed, a, novel, mutu...   \n",
            "3  [consistency, training, has, proven, to, be, a...   \n",
            "4  [to, ensure, safety, in, automated, driving, t...   \n",
            "\n",
            "                                       filtered_news  \n",
            "0  [stereo, matching, one, widely, used, techniqu...  \n",
            "1  [recent, advancements, artificial, intelligenc...  \n",
            "2  [paper, proposed, novel, mutual, consistency, ...  \n",
            "3  [consistency, training, proven, advanced, semi...  \n",
            "4  [ensure, safety, automated, driving, correct, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LEMMATIZATION"
      ],
      "metadata": {
        "id": "63UqTHP-_CIb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4zJbILEV7BJ",
        "outputId": "1edfaef2-c263-44d5-de34-5bde97afd4a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                       filtered_news  \\\n",
            "0  [stereo, matching, one, widely, used, techniqu...   \n",
            "1  [recent, advancements, artificial, intelligenc...   \n",
            "2  [paper, proposed, novel, mutual, consistency, ...   \n",
            "3  [consistency, training, proven, advanced, semi...   \n",
            "4  [ensure, safety, automated, driving, correct, ...   \n",
            "\n",
            "                                     lemmatized_news  \n",
            "0  [stereo, matching, one, widely, used, techniqu...  \n",
            "1  [recent, advancement, artificial, intelligence...  \n",
            "2  [paper, proposed, novel, mutual, consistency, ...  \n",
            "3  [consistency, training, proven, advanced, semi...  \n",
            "4  [ensure, safety, automated, driving, correct, ...  \n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Ensure the WordNet corpus is available\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # optional, improves lemmatization coverage\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply lemmatization to each token list\n",
        "df1['lemmatized_news'] = df1['filtered_news'].apply(\n",
        "    lambda tokens: [lemmatizer.lemmatize(w) for w in tokens]\n",
        ")\n",
        "\n",
        "# Preview results\n",
        "print(df1[['filtered_news', 'lemmatized_news']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BOW"
      ],
      "metadata": {
        "id": "8f0k-0GD_ebJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc219e79"
      },
      "source": [
        "# Task\n",
        "Perform LDA topic modeling on the `summaries` column of the `arxiv_data.csv` dataset. This involves cleaning the text by removing URLs, HTML tags, social media mentions, hashtags, emojis, and special characters, converting text to lowercase, normalizing whitespace, then tokenizing, removing stopwords, and lemmatizing. After preprocessing, create a Bag-of-Words representation, prepare the corpus for LDA, apply an LDA model to identify topics, and finally, display the top words for each identified topic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "856b26e4"
      },
      "source": [
        "## Load Text Data\n",
        "\n",
        "### Subtask:\n",
        "Load the text data from the provided source (e.g., a CSV file) into a pandas DataFrame. This will typically involve loading the `arxiv_data.csv`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81c11f85"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask explicitly asks to load the text data from `arxiv_data.csv` into a DataFrame named `df1` and display its head. Although this step has appeared before, re-executing it ensures the `df1` DataFrame is correctly initialized as per the task requirements, which is a necessary precursor for subsequent processing steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab19bd5d",
        "outputId": "2f90b3ec-cb94-4cb7-f567-aa5dcef2e469"
      },
      "source": [
        "import pandas as pd\n",
        "df1=pd.read_csv('/content/arxiv_data.csv')\n",
        "print(df1.head())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              titles  \\\n",
            "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
            "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
            "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
            "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
            "4  Background-Foreground Segmentation for Interio...   \n",
            "\n",
            "                                           summaries  \\\n",
            "0  Stereo matching is one of the widely used tech...   \n",
            "1  The recent advancements in artificial intellig...   \n",
            "2  In this paper, we proposed a novel mutual cons...   \n",
            "3  Consistency training has proven to be an advan...   \n",
            "4  To ensure safety in automated driving, the cor...   \n",
            "\n",
            "                         terms  \n",
            "0           ['cs.CV', 'cs.LG']  \n",
            "1  ['cs.CV', 'cs.AI', 'cs.LG']  \n",
            "2           ['cs.CV', 'cs.AI']  \n",
            "3                    ['cs.CV']  \n",
            "4           ['cs.CV', 'cs.LG']  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9898c0db"
      },
      "source": [
        "## Text Preprocessing - Clean Text\n",
        "\n",
        "### Subtask:\n",
        "Apply a series of text cleaning steps to the 'summaries' column, including removing URLs, HTML tags, social media mentions, hashtags, emojis, special characters, converting text to lowercase, and normalizing whitespace. Store the cleaned text in a new column.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4888fa3d"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires applying a series of text cleaning steps to the 'summaries' column. This involves importing the `re` module, defining a `preprocess_text` function to handle various cleaning operations like removing URLs, HTML tags, social media mentions, hashtags, emojis, special characters, converting to lowercase, and normalizing whitespace. Finally, the function will be applied to the 'summaries' column to create a new 'processed_news' column, and the head of both columns will be displayed to verify the cleaning process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a2cf7a8",
        "outputId": "d4a81cdc-1b63-4539-ba2d-2f01c44f693d"
      },
      "source": [
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove URLs (http, https, www)\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove social media mentions (@username)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Remove hashtags (#hashtag)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove emojis\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    # Remove special characters (keep only alphanumeric and spaces)\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "\n",
        "    # Normalize whitespace (reduce multiple spaces to single space)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to 'summaries' column\n",
        "df1['processed_news'] = df1['summaries'].apply(preprocess_text)\n",
        "\n",
        "# Preview results\n",
        "print(df1[['summaries', 'processed_news']].head())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           summaries  \\\n",
            "0  Stereo matching is one of the widely used tech...   \n",
            "1  The recent advancements in artificial intellig...   \n",
            "2  In this paper, we proposed a novel mutual cons...   \n",
            "3  Consistency training has proven to be an advan...   \n",
            "4  To ensure safety in automated driving, the cor...   \n",
            "\n",
            "                                      processed_news  \n",
            "0  stereo matching is one of the widely used tech...  \n",
            "1  the recent advancements in artificial intellig...  \n",
            "2  in this paper we proposed a novel mutual consi...  \n",
            "3  consistency training has proven to be an advan...  \n",
            "4  to ensure safety in automated driving the corr...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82a3cce6"
      },
      "source": [
        "## Text Preprocessing - Tokenization\n",
        "\n",
        "### Subtask:\n",
        "Tokenize the preprocessed text in the 'processed_news' column into individual words. Store the tokenized words in a new column called 'tokenized_news'.\n",
        "\n",
        "#### Instructions:\n",
        "1. Import necessary libraries: `nltk` and `word_tokenize` from `nltk.tokenize`.\n",
        "2. Download the 'punkt' tokenizer models if not already present.\n",
        "3. Apply `word_tokenize` to the 'processed_news' column and store the result in a new column named 'tokenized_news'.\n",
        "4. Display the first five rows of the 'processed_news' and 'tokenized_news' columns to verify the tokenization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "533d8cc5"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires tokenizing the preprocessed text. This involves importing `nltk` and `word_tokenize`, ensuring the 'punkt' tokenizer models are downloaded, applying `word_tokenize` to the `processed_news` column, and storing the result in a new `tokenized_news` column. Finally, the head of both columns will be displayed to verify the tokenization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b36338ed",
        "outputId": "617857fd-641d-40d5-c50d-9d2975c9dee4"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')  # required in newer NLTK versions\n",
        "\n",
        "df1['tokenized_news'] = df1['processed_news'].apply(lambda x: word_tokenize(x))\n",
        "print(df1[['processed_news', 'tokenized_news']].head())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      processed_news  \\\n",
            "0  stereo matching is one of the widely used tech...   \n",
            "1  the recent advancements in artificial intellig...   \n",
            "2  in this paper we proposed a novel mutual consi...   \n",
            "3  consistency training has proven to be an advan...   \n",
            "4  to ensure safety in automated driving the corr...   \n",
            "\n",
            "                                      tokenized_news  \n",
            "0  [stereo, matching, is, one, of, the, widely, u...  \n",
            "1  [the, recent, advancements, in, artificial, in...  \n",
            "2  [in, this, paper, we, proposed, a, novel, mutu...  \n",
            "3  [consistency, training, has, proven, to, be, a...  \n",
            "4  [to, ensure, safety, in, automated, driving, t...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeb7c533"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully tokenized the text. The next logical step in text preprocessing for topic modeling is to remove common stopwords, as these words often do not contribute meaningful information to topic identification. This will be followed by a reasoning block and the code block for stop word removal.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eae4711"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block failed because it contained markdown instructions within a Python code cell. The intent was to perform stop word removal, so this code block will provide the correct Python implementation for that task, assuming the markdown instructions will be placed in a separate markdown cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "147233da",
        "outputId": "ed94899f-19bf-4092-9ad9-a906d4da5661"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords') # Ensure stopwords is downloaded\n",
        "\n",
        "# Define the set of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords from tokenized_news\n",
        "df1['filtered_news'] = df1['tokenized_news'].apply(\n",
        "    lambda tokens: [w for w in tokens if w.lower() not in stop_words]\n",
        ")\n",
        "\n",
        "print(df1[['tokenized_news', 'filtered_news']].head())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      tokenized_news  \\\n",
            "0  [stereo, matching, is, one, of, the, widely, u...   \n",
            "1  [the, recent, advancements, in, artificial, in...   \n",
            "2  [in, this, paper, we, proposed, a, novel, mutu...   \n",
            "3  [consistency, training, has, proven, to, be, a...   \n",
            "4  [to, ensure, safety, in, automated, driving, t...   \n",
            "\n",
            "                                       filtered_news  \n",
            "0  [stereo, matching, one, widely, used, techniqu...  \n",
            "1  [recent, advancements, artificial, intelligenc...  \n",
            "2  [paper, proposed, novel, mutual, consistency, ...  \n",
            "3  [consistency, training, proven, advanced, semi...  \n",
            "4  [ensure, safety, automated, driving, correct, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bc8dfbb"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully removed stopwords. The next logical step in text preprocessing for topic modeling is lemmatization, which reduces words to their base or dictionary form, further normalizing the text. This will be followed by a reasoning block and the code block for lemmatization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2c97982",
        "outputId": "371c88f6-d41a-46f2-858f-f00639e0dd39"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Ensure the WordNet corpus is available\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # optional, improves lemmatization coverage\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply lemmatization to each token list\n",
        "df1['lemmatized_news'] = df1['filtered_news'].apply(\n",
        "    lambda tokens: [lemmatizer.lemmatize(w) for w in tokens]\n",
        ")\n",
        "\n",
        "# Preview results\n",
        "print(df1[['filtered_news', 'lemmatized_news']].head())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                       filtered_news  \\\n",
            "0  [stereo, matching, one, widely, used, techniqu...   \n",
            "1  [recent, advancements, artificial, intelligenc...   \n",
            "2  [paper, proposed, novel, mutual, consistency, ...   \n",
            "3  [consistency, training, proven, advanced, semi...   \n",
            "4  [ensure, safety, automated, driving, correct, ...   \n",
            "\n",
            "                                     lemmatized_news  \n",
            "0  [stereo, matching, one, widely, used, techniqu...  \n",
            "1  [recent, advancement, artificial, intelligence...  \n",
            "2  [paper, proposed, novel, mutual, consistency, ...  \n",
            "3  [consistency, training, proven, advanced, semi...  \n",
            "4  [ensure, safety, automated, driving, correct, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d524ce3"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully lemmatized the text. The next logical step for preparing text for LDA is to rejoin the lemmatized tokens into a single string for each document. This aggregated string will serve as the clean text input for further vectorization or model training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8045bc6c"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block failed with a `SyntaxError` because it contained markdown instructions within a Python code cell. The intent was to rejoin the lemmatized tokens into a single string for each document, so this code block will provide the correct Python implementation for that task, separating the instructional text from the Python code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a0a88aa",
        "outputId": "8a10f932-341c-49f4-8dd8-68cbe3a0fb71"
      },
      "source": [
        "df1['clean_news'] = df1['lemmatized_news'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "# Preview results\n",
        "print(df1[['lemmatized_news', 'clean_news']].head())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     lemmatized_news  \\\n",
            "0  [stereo, matching, one, widely, used, techniqu...   \n",
            "1  [recent, advancement, artificial, intelligence...   \n",
            "2  [paper, proposed, novel, mutual, consistency, ...   \n",
            "3  [consistency, training, proven, advanced, semi...   \n",
            "4  [ensure, safety, automated, driving, correct, ...   \n",
            "\n",
            "                                          clean_news  \n",
            "0  stereo matching one widely used technique infe...  \n",
            "1  recent advancement artificial intelligence ai ...  \n",
            "2  paper proposed novel mutual consistency networ...  \n",
            "3  consistency training proven advanced semisuper...  \n",
            "4  ensure safety automated driving correct percep...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10f093fa"
      },
      "source": [
        "## Bag-of-Words Representation\n",
        "\n",
        "### Subtask:\n",
        "Create a Bag-of-Words (BOW) representation from the `clean_news` column using `CountVectorizer`. Display the vocabulary and the BOW matrix.\n",
        "\n",
        "#### Instructions:\n",
        "1. Import `CountVectorizer` from `sklearn.feature_extraction.text`.\n",
        "2. Initialize `CountVectorizer`.\n",
        "3. Fit and transform the `clean_news` column to get the BOW matrix.\n",
        "4. Convert the BOW matrix to an array for easier viewing.\n",
        "5. Print the vocabulary (feature names) obtained from the vectorizer.\n",
        "6. Print the generated Bag-of-Words matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77ef2339"
      },
      "source": [
        "## Bag-of-Words Representation\n",
        "\n",
        "### Subtask:\n",
        "Create a Bag-of-Words (BOW) representation from the `clean_news` column using `CountVectorizer`. Display the vocabulary and the BOW matrix.\n",
        "\n",
        "#### Instructions:\n",
        "1. Import `CountVectorizer` from `sklearn.feature_extraction.text`.\n",
        "2. Initialize `CountVectorizer`.\n",
        "3. Fit and transform the `clean_news` column to get the BOW matrix.\n",
        "4. Convert the BOW matrix to an array for easier viewing.\n",
        "5. Print the vocabulary (feature names) obtained from the vectorizer.\n",
        "6. Print the generated Bag-of-Words matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90e3ff85"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous markdown block outlined the steps to create a Bag-of-Words representation. This code block will implement those steps by importing `CountVectorizer`, initializing it, fitting and transforming the `clean_news` column, and then printing the vocabulary and the resulting BOW matrix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c8627fa"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform sentences\n",
        "bow_matrix = vectorizer.fit_transform(df1['clean_news'])\n",
        "\n",
        "# Convert to array for readability\n",
        "bow_array = bow_matrix.toarray()\n",
        "\n",
        "# Show vocabulary\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "\n",
        "# Show Bag-of-Words matrix\n",
        "print(\"Bag-of-Words Matrix:\\n\", bow_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e004cdf"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous markdown block outlined the steps to create a Bag-of-Words representation. This code block will implement those steps by importing `CountVectorizer`, initializing it, fitting and transforming the `clean_news` column, and then printing the vocabulary and the resulting BOW matrix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34fd2404",
        "outputId": "739c0dd1-9dce-4656-fb3f-2aa63199860c"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# 1. Load data to ensure df1 is defined\n",
        "df1 = pd.read_csv('/content/arxiv_data.csv')\n",
        "\n",
        "# 2. Define and apply preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove URLs (http, https, www)\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove social media mentions (@username)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Remove hashtags (#hashtag)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove emojis\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    # Remove special characters (keep only alphanumeric and spaces)\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "\n",
        "    # Normalize whitespace (reduce multiple spaces to single space)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "df1['processed_news'] = df1['summaries'].apply(preprocess_text)\n",
        "\n",
        "# 3. Tokenization\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "df1['tokenized_news'] = df1['processed_news'].apply(lambda x: word_tokenize(x))\n",
        "\n",
        "# 4. Stop word removal\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df1['filtered_news'] = df1['tokenized_news'].apply(\n",
        "    lambda tokens: [w for w in tokens if w.lower() not in stop_words]\n",
        ")\n",
        "\n",
        "# 5. Lemmatization\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df1['lemmatized_news'] = df1['filtered_news'].apply(\n",
        "    lambda tokens: [lemmatizer.lemmatize(w) for w in tokens]\n",
        ")\n",
        "\n",
        "# 6. Rejoin clean text\n",
        "df1['clean_news'] = df1['lemmatized_news'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "# 7. Create Bag-of-Words representation\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(df1['clean_news'])\n",
        "bow_array = bow_matrix.toarray()\n",
        "\n",
        "# Show vocabulary\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "\n",
        "# Show Bag-of-Words matrix\n",
        "print(\"Bag-of-Words Matrix:\\n\", bow_array)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5d57866"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block failed because the `df1` DataFrame and its processed column `clean_news` were not defined in the current execution context. This fix re-initializes `df1` by loading the data and re-applies all necessary preprocessing steps (cleaning, tokenization, stop word removal, lemmatization, and rejoining) to ensure `clean_news` is available before `CountVectorizer` is applied, thus resolving the `NameError`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "723c76ec",
        "outputId": "2df92538-85a0-4550-89c1-4c9094ee0f29"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# 1. Load data to ensure df1 is defined\n",
        "df1 = pd.read_csv('/content/arxiv_data.csv')\n",
        "\n",
        "# 2. Define and apply preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove URLs (http, https, www)\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove social media mentions (@username)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Remove hashtags (#hashtag)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove emojis\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    # Remove special characters (keep only alphanumeric and spaces)\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "\n",
        "    # Normalize whitespace (reduce multiple spaces to single space)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "df1['processed_news'] = df1['summaries'].apply(preprocess_text)\n",
        "\n",
        "# 3. Tokenization\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "df1['tokenized_news'] = df1['processed_news'].apply(lambda x: word_tokenize(x))\n",
        "\n",
        "# 4. Stop word removal\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df1['filtered_news'] = df1['tokenized_news'].apply(\n",
        "    lambda tokens: [w for w in tokens if w.lower() not in stop_words]\n",
        ")\n",
        "\n",
        "# 5. Lemmatization\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df1['lemmatized_news'] = df1['filtered_news'].apply(\n",
        "    lambda tokens: [lemmatizer.lemmatize(w) for w in tokens]\n",
        ")\n",
        "\n",
        "# 6. Rejoin clean text\n",
        "df1['clean_news'] = df1['lemmatized_news'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "# 7. Create Bag-of-Words representation\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(df1['clean_news'])\n",
        "bow_array = bow_matrix.toarray()\n",
        "\n",
        "# Show vocabulary\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "\n",
        "# Show Bag-of-Words matrix\n",
        "print(\"Bag-of-Words Matrix:\\n\", bow_array)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1653075f"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block successfully created the Bag-of-Words representation. The next step, as per the overall task, is to prepare the corpus for LDA using `gensim.corpora.Dictionary` and `doc2bow` from the `lemmatized_news` column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86f224ed"
      },
      "source": [
        "## Prepare Corpus for LDA\n",
        "\n",
        "### Subtask:\n",
        "Install the `gensim` library and then create a Gensim dictionary and corpus (bag-of-words format) from the lemmatized and tokenized text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad1123cd"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to import `gensim` failed because the library was not installed. This step will install the `gensim` library using `!pip install gensim` so that it can be imported and used in subsequent steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d7ca6c2",
        "outputId": "e52b28fe-1194-41cb-ab40-706e794038d3"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9c73bfb"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that `gensim` is installed, I will import `gensim.corpora`, create a dictionary from the `lemmatized_news` column of `df1`, then create the corpus using `doc2bow`, and finally print verification details as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "052a3eb8"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block failed because `df1` was not defined in the current execution context. This fix re-initializes `df1` and re-applies all necessary preprocessing steps to create the `lemmatized_news` column before proceeding with the `gensim` dictionary and corpus creation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Example dataset (you can replace this with your Excel file)\n",
        "data = {\n",
        "    \"Sentence\": [\n",
        "        \"The cat sat on the mat\",\n",
        "        \"She loves music\",\n",
        "        \"Apples are red\",\n",
        "        \"The car engine broke down\"\n",
        "    ]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Extract sentences\n",
        "sentences = df[\"Sentence\"].astype(str).tolist()\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Fit and transform sentences\n",
        "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "# Convert to DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Add original sentences for reference\n",
        "tfidf_df.insert(0, \"Sentence\", sentences)\n",
        "\n",
        "# Preview\n",
        "print(tfidf_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdqiul7ssyXG",
        "outputId": "78bd091c-8692-4857-b13c-331212124a77"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    Sentence    apples    broke      car      cat   engine  \\\n",
            "0     The cat sat on the mat  0.000000  0.00000  0.00000  0.57735  0.00000   \n",
            "1            She loves music  0.000000  0.00000  0.00000  0.00000  0.00000   \n",
            "2             Apples are red  0.707107  0.00000  0.00000  0.00000  0.00000   \n",
            "3  The car engine broke down  0.000000  0.57735  0.57735  0.00000  0.57735   \n",
            "\n",
            "      loves      mat     music       red      sat  \n",
            "0  0.000000  0.57735  0.000000  0.000000  0.57735  \n",
            "1  0.707107  0.00000  0.707107  0.000000  0.00000  \n",
            "2  0.000000  0.00000  0.000000  0.707107  0.00000  \n",
            "3  0.000000  0.00000  0.000000  0.000000  0.00000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Example dataset (replace with your Excel file if needed)\n",
        "data = {\n",
        "    \"Sentence\": [\n",
        "        \"The cat sat on the mat\",\n",
        "        \"She loves music\",\n",
        "        \"Apples are red\",\n",
        "        \"The car engine broke down\"\n",
        "    ]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Extract sentences\n",
        "sentences = df[\"Sentence\"].astype(str).tolist()\n",
        "\n",
        "# Initialize CountVectorizer (BoW)\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "# Fit and transform sentences\n",
        "bow_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "# Convert to DataFrame\n",
        "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Add original sentences for reference\n",
        "bow_df.insert(0, \"Sentence\", sentences)\n",
        "\n",
        "# Preview\n",
        "print(bow_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4RRb2SXtEyZ",
        "outputId": "b7dfbcaf-7ab2-41c5-da61-69fa955c9107"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    Sentence  apples  broke  car  cat  engine  loves  mat  \\\n",
            "0     The cat sat on the mat       0      0    0    1       0      0    1   \n",
            "1            She loves music       0      0    0    0       0      1    0   \n",
            "2             Apples are red       1      0    0    0       0      0    0   \n",
            "3  The car engine broke down       0      1    1    0       1      0    0   \n",
            "\n",
            "   music  red  sat  \n",
            "0      0    0    1  \n",
            "1      1    0    0  \n",
            "2      0    1    0  \n",
            "3      0    0    0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "# Example dataset (replace with your Excel file)\n",
        "data = {\n",
        "    \"Sentence\": [\n",
        "        \"The cat sat on the mat\",\n",
        "        \"She loves music\",\n",
        "        \"Apples are red\",\n",
        "        \"The car engine broke down\"\n",
        "    ]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Step 1: Prepare corpus\n",
        "sentences = df[\"Sentence\"].astype(str).tolist()\n",
        "\n",
        "# Step 2: TF-IDF representation\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "# Step 3: Apply NMF\n",
        "num_topics = 2   # you can adjust this\n",
        "nmf_model = NMF(n_components=num_topics, random_state=42)\n",
        "nmf_model.fit(tfidf_matrix)\n",
        "\n",
        "# Step 4: Identify words for each topic\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "num_words = 5\n",
        "\n",
        "for topic_idx, topic in enumerate(nmf_model.components_):\n",
        "    print(f\"\\nTopic {topic_idx}:\")\n",
        "    top_indices = topic.argsort()[-num_words:][::-1]\n",
        "    for i in top_indices:\n",
        "        print(f\"  {feature_names[i]} ({topic[i]:.3f})\")\n",
        "\n",
        "# Step 5: Assign topics to sentences\n",
        "doc_topic_matrix = nmf_model.transform(tfidf_matrix)\n",
        "df[\"Topic\"] = doc_topic_matrix.argmax(axis=1)\n",
        "\n",
        "print(\"\\nSentence-to-topic mapping:\")\n",
        "print(df[[\"Sentence\", \"Topic\"]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAN9J9R2tGwk",
        "outputId": "77628ba2-370f-4ede-b98e-81a135d4ed22"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Topic 0:\n",
            "  car (0.683)\n",
            "  engine (0.683)\n",
            "  broke (0.683)\n",
            "  music (0.205)\n",
            "  loves (0.205)\n",
            "\n",
            "Topic 1:\n",
            "  sat (0.671)\n",
            "  mat (0.671)\n",
            "  cat (0.671)\n",
            "  loves (0.000)\n",
            "  music (0.000)\n",
            "\n",
            "Sentence-to-topic mapping:\n",
            "                    Sentence  Topic\n",
            "0     The cat sat on the mat      1\n",
            "1            She loves music      0\n",
            "2             Apples are red      0\n",
            "3  The car engine broke down      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uylwHNThtV94"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPG5LEo6ARl4Ol2gxGXXXwj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}